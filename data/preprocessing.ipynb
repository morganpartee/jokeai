{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load\n",
    "\n",
    "titles, jokes = [], []\n",
    "\n",
    "files = [\"reddit_jokes.json\", \"stupidstuff.json\", \"wocka.json\"]\n",
    "for file_name in files:\n",
    "    with open(file_name) as f:\n",
    "        for entry in load(f):\n",
    "            if \"body\" in entry:\n",
    "                jokes.append(entry.get(\"body\"))\n",
    "            if \"title\" in entry:\n",
    "                titles.append(entry.get(\"title\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"titles.txt\", 'w') as f:\n",
    "    f.writelines(titles)\n",
    "with open(\"jokes.txt\", 'w') as f:\n",
    "    f.writelines(jokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/john/jokeai/ml/joke-dataset/preprocessing.ipynb Cell 5'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/john/jokeai/ml/joke-dataset/preprocessing.ipynb#ch0000006?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mget_device_name(\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:326\u001b[0m, in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=313'>314</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=314'>315</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Gets the name of a device.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=315'>316</a>\u001b[0m \n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=316'>317</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=323'>324</a>\u001b[0m \u001b[39m        str: the name of the device\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=324'>325</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=325'>326</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m get_device_properties(device)\u001b[39m.\u001b[39mname\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:356\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=345'>346</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_device_properties\u001b[39m(device: _device_t) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m _CudaDeviceProperties:\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=346'>347</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Gets the properties of a device.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=347'>348</a>\u001b[0m \n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=348'>349</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=353'>354</a>\u001b[0m \u001b[39m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=354'>355</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=355'>356</a>\u001b[0m     _lazy_init()  \u001b[39m# will define _get_device_properties\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=356'>357</a>\u001b[0m     device \u001b[39m=\u001b[39m _get_device_index(device, optional\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=357'>358</a>\u001b[0m     \u001b[39mif\u001b[39;00m device \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count():\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:214\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=209'>210</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=210'>211</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=211'>212</a>\u001b[0m \u001b[39m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=212'>213</a>\u001b[0m \u001b[39m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=213'>214</a>\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=214'>215</a>\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=215'>216</a>\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=216'>217</a>\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py?line=217'>218</a>\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: markovify in /home/john/.local/lib/python3.10/site-packages (0.9.3)\n",
      "Requirement already satisfied: unidecode in /home/john/.local/lib/python3.10/site-packages (from markovify) (1.3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not surprisingly, the monks packed up the very next female to walk into the kitchen.\n",
      "John pauses for quite a while in a detailed examination.\n",
      "To test the family he gets up and walks to the front door.\n",
      "Everyone in the room was wondering what was given to the French!Billy Bob wanted a job as a plumber and his life significantly improved.\n",
      "All the while, I was trying to keep up with the Joneses.\n"
     ]
    }
   ],
   "source": [
    "import markovify\n",
    "\n",
    "# Get raw text as string.\n",
    "with open(\"jokes.txt\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Build the model.\n",
    "text_model = markovify.Text(text, state_size=4)\n",
    "text_model.compile()\n",
    "\n",
    "# Print five randomly-generated sentences\n",
    "for i in range(5):\n",
    "    print(text_model.make_sentence())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading LanguageTool 5.6: 100%|██████████| 220M/220M [00:24<00:00, 9.01MB/s] \n",
      "Unzipping /tmp/tmpmlbng0dt.zip to /home/john/.cache/language_tool_python.\n",
      "Downloaded https://www.languagetool.org/download/LanguageTool-5.6.zip to /home/john/.cache/language_tool_python.\n"
     ]
    }
   ],
   "source": [
    "import language_tool_python\n",
    "tool = language_tool_python.LanguageTool('en-US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient: then we'll kiss and hug her, hold her, sensual like and they have rain over every aspect of production, including colour  or lack thereof.\n",
      "Patient: then we'll kiss and hug her, hold her, sensual like, and they have rain over every aspect of production, including color  or lack thereof.\n"
     ]
    }
   ],
   "source": [
    "joke = text_model.make_sentence()\n",
    "print(joke)\n",
    "print(tool.correct(joke))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
